# ğŸš€ Local AI Chatbot â€” Chainlit + Ollama (llama3.2)

This project is a fully local AI chatbot built using **Chainlit** for the UI and **Ollama** for running the `llama3.2:latest` model.  
It provides a simple, fast, and private alternative to ChatGPT â€” all running on your own machine.

---

## âœ¨ Features

- ğŸ’¬ **Interactive chat UI** powered by Chainlit  
- âš¡ **Streaming responses** for a smooth chat experience  
- ğŸ” **100% local** â€” no API keys or cloud services  
- ğŸ¦™ Uses **Ollama** to run `llama3.2:latest` locally  
- ğŸ“š Session-based message handling  
- ğŸ¯ Very simple to run and extend

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ app.py # Main application file
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

yaml
Copy code

---

## ğŸ› ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git
cd YOUR_REPOSITORY_NAME
2ï¸âƒ£ Create & activate a virtual environment (optional but recommended)
bash
Copy code
python -m venv venv
source venv/bin/activate      # Linux / macOS
venv\Scripts\activate         # Windows
3ï¸âƒ£ Install dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Install Ollama
Download from:
https://ollama.com/download

Then pull the required model:

bash
Copy code
ollama pull llama3.2:latest
â–¶ï¸ Running the App
Start the Chainlit app using:

bash
Copy code
chainlit run app.py -w
Your local chatbot UI will now run at:

ğŸ‘‰ http://localhost:8000

ğŸ§  How It Works
When the chat starts, a greeting message is streamed character-by-character.

Messages are stored in a user_session list.

Each user message is sent to Ollama using the llama3.2:latest model.

Responses are streamed back to the UI.

Chainlit handles the interface and step execution.

ğŸ¤ Contributing
Feel free to open issues or submit pull requests to improve the app!

ğŸ“œ License
This project is released under the MIT License.
